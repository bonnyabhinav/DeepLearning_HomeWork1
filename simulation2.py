# -*- coding: utf-8 -*-
"""simulation2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xYuSYC0WhGajxo5w81plrOOGm31M5FJH
"""

import tensorflow as tf
import numpy as np
import math
import matplotlib.pyplot as plt

def parameters():
    total_parameters = 0
    for variable in tf.trainable_variables():
      # shape is an array of tf.Dimension
      print(variable)
      shape = variable.get_shape()
      print(shape)
      #print(len(shape))
      variable_parameters = 1
      for dim in shape:
        #print(dim)
        variable_parameters *= dim.value
      print(variable_parameters)
      total_parameters += variable_parameters
    print(total_parameters)
    return True

def loss_plotting(loss):
  plt.plot(loss)
  plt.title('Loss at each Epoch')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.show()
  return True

def grad_plotting(grad):
  plt.plot(grad)
  plt.title('Loss at each Epoch')
  plt.xlabel('Epochs')
  plt.ylabel('Grad')
  plt.show()
  return True

def norm_grad(gradients):
  grad_all =0.0
  for row in gradients:
    grad = 0.0
    for element in row.flat:
      grad = (element ** 2).sum()
      grad_all += grad

  grad_norm = grad_all ** 0.5
  return grad_norm

def grad_loss_plot(grad, loss):
  fig1 = plt.figure(figsize=(6,6))
  ax1 = fig1.add_subplot(1,1,1)
  ax1.set_xlabel('Iteration', fontsize = 15)
  ax1.set_ylabel('Gradient', fontsize = 15)
  ax1.set_title('Observe Gradient', fontsize = 20)
  ax1.plot(grad)
  fig2 = plt.figure(figsize=(6,6))
  ax2 = fig2.add_subplot(1,1,1)
  ax2.set_xlabel('Iteration', fontsize = 15)
  ax2.set_ylabel('Loss', fontsize = 15)
  ax2.set_title('Observe Loss', fontsize = 20)
  ax2.plot(loss)

sess = tf.InteractiveSession()
X =np.expand_dims(np.arange(0.0, 3.0, 0.01),1)
Y =((np.sinc(5*np.pi*X))/5*np.pi*X)
x = tf.placeholder(tf.float64, [300,1], name='x')
y = tf.placeholder(tf.float64, [300,1], name='y')

input_layer = tf.layers.dense(x, 240, activation= tf.nn.tanh)
hidden_layer1 = tf.layers.dense(input_layer,16, activation= tf.nn.tanh)
output_layer = tf.layers.dense(hidden_layer1,1)
Loss =tf.losses.mean_squared_error(y , output_layer)
Optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(Loss)
init = tf.global_variables_initializer()
parameters()

loss_list=[]
gradient=[]
grad_list=[]
sess.run(init)
grads = tf.gradients(Loss,output_layer)[0]

for i in range(0,20000):
  fd ={x:X, y:Y}
  _, grad,loss_val = sess.run([Optimizer,grads, Loss], feed_dict=fd)
  print ('loss = %s' % loss_val)
  loss_list.append(loss_val)
  gradients = sess.run(tf.cast(grad, tf.float32))
  gradient_norms = norm_grad(gradients)
  grad_list.append(gradient_norms)
  gradient.append(gradients)

YP = sess.run(output_layer,feed_dict={x:X})
grad_loss_plot(grad_list, loss_list)

# Plot training  loss values
import matplotlib.pyplot as plt
plt.plot(loss_list)
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(X,Y)
plt.plot(X,YP)
plt.legend(['Ground Truth', 'Predicted'])
plt.show()