# -*- coding: utf-8 -*-
"""sim_model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F5S0L1mld-ow9wSoL5f8Jh3jIxTLA8Nm
"""

import tensorflow as tf
import numpy as np
import math
import matplotlib.pyplot as plt

def parameters():
    total_parameters = 0
    for variable in tf.trainable_variables():
      # shape is an array of tf.Dimension
      print(variable)
      shape = variable.get_shape()
      print(shape)
      #print(len(shape))
      variable_parameters = 1
      for dim in shape:
        #print(dim)
        variable_parameters *= dim.value
      print(variable_parameters)
      total_parameters += variable_parameters
    print(total_parameters)
    return True

def loss_plotting(loss):
  plt.plot(loss)
  plt.title('Loss at each Epoch')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.show()
  return True

def grad_plotting(grad):
  plt.plot(grad)
  plt.title('Loss at each Epoch')
  plt.xlabel('Epochs')
  plt.ylabel('Grad')
  plt.show()
  return True

def norm_grad(gradients):
  grad_all =0.0
  for row in gradients:
    grad = 0.0
    for element in row.flat:
      grad = (element ** 2).sum()
      grad_all += grad

  grad_norm = grad_all ** 0.5
  return grad_norm

def grad_loss_plot(grad, loss):
  fig1 = plt.figure(figsize=(6,6))
  ax1 = fig1.add_subplot(1,1,1)
  ax1.set_xlabel('Iteration', fontsize = 15)
  ax1.set_ylabel('Gradient', fontsize = 15)
  ax1.set_title('Observe Gradient', fontsize = 20)
  ax1.plot(grad)
  fig2 = plt.figure(figsize=(6,6))
  ax2 = fig2.add_subplot(1,1,1)
  ax2.set_xlabel('Iteration', fontsize = 15)
  ax2.set_ylabel('Loss', fontsize = 15)
  ax2.set_title('Observe Loss', fontsize = 20)
  ax2.plot(loss)

def pred_vs_truth(X,pred,orig):
  plt.plot(X,pred)
  plt.plot(X,orig)
  plt.legend(['Predicted','Ground Truth'])
  plt.title('Prdicted vs Ground Truth')
  plt.xlabel('Ground Truth')
  plt.ylabel('Predicted')
  plt.show()
  return True



sess = tf.InteractiveSession()
X =np.expand_dims(np.arange(0.0, 3.0, 0.01),1)
Y =((np.sinc(5*np.pi*X))/5*np.pi*X)
x = tf.placeholder(tf.float64, [300,1], name='x')
y = tf.placeholder(tf.float64, [300,1], name='y')

#input_layer = tf.layers.dense(x, 240, activation= tf.nn.tanh)
hidden_layer1 = tf.layers.dense(inputs=x,units = 300, activation= tf.nn.tanh)
output_layer = tf.layers.dense(hidden_layer1,units=1)
Loss =tf.losses.mean_squared_error(y , output_layer)
Optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(Loss)
init = tf.global_variables_initializer()
parameters()

loss_list=[]
gradient=[]
grad_list=[]
sess.run(init)
grads = tf.gradients(Loss,output_layer)[0]

for i in range(0,1000):
  fd ={x:X, y:Y}
  _, grad,loss_val = sess.run([Optimizer,grads, Loss], feed_dict=fd)
  print ('loss = %s' % loss_val)
  loss_list.append(loss_val)
  gradients = sess.run(tf.cast(grad, tf.float32))
  gradient_norms = norm_grad(gradients)
  grad_list.append(gradient_norms)
  gradient.append(gradients)

pred = sess.run(output_layer,feed_dict={x:X})
pred_vs_truth(X,pred,Y)

grad_loss_plot(grad_list, loss_list)