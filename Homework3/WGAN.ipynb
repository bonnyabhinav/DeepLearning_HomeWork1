{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbaTiphAjP2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXw8a3K5jQWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKfFXGb9jQZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_dataset = tf.data.Dataset.from_tensor_slices(mnist_train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "def training_batches():\n",
        "  path = \"C:\\\\Users\\\\abhin\\\\Downloads\\\\cifar-10-batches-py\"\n",
        "  file = \"data_batch_1\"\n",
        "  #file = pd.concat(file,next)\n",
        "\n",
        "  #file_path = os.path.join(path,file)\n",
        "\n",
        "  with open(file, 'rb') as fo:\n",
        "    training_batch = pickle.load(fo, encoding='bytes')  #retruns a dictionary\n",
        "  \n",
        "  features = training_batch[b'data']\n",
        "  labels = training_batch[b'labels']\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKh6Xk3yjQmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WGAN():\n",
        "  def __init__(self):\n",
        "    self.batch_size = 10000\n",
        "    self.image_size = 32*32\n",
        "    self.image_channels = 3\n",
        "  \n",
        "  def inputs(self):\n",
        "    x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
        "    z = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='noise')\n",
        "    prob = tf.placeholder(tf.float32, name='prob')\n",
        "\n",
        "    return x, z, prob\n",
        "  \n",
        "  def wgan_critic_model():\n",
        "    with tf.variable_scope(\"discriminator\") as scope:\n",
        "      if reuse:\n",
        "        scope.reuse_variables()\n",
        "\n",
        "      wt = tf.truncated_normal_initializer(stddev=0.02)\n",
        "      d_wt1 = tf.random_normal_initializer(stddev=0.02)\n",
        "      d_wt2 = tf.random_normal_initializer(stddev=0.02)\n",
        "      d_wt3 = tf.random_normal_initializer(stddev=0.02)\n",
        "      d_wt4 = tf.random_normal_initializer(stddev=0.02)\n",
        "\n",
        "    #First Convoultion Layer\n",
        "    d_conv1 = tf.nn.conv2d(input, (5,5), strides = [1, 2, 2, 1], padding='SAME')\n",
        "    d_conv1 = tf.nn.leaky_relu(d_conv1, alpha = 0.2)\n",
        "\n",
        "    #Second Convolution Layer\n",
        "    d_conv2 = tf.nn.conv2d(d_conv1, (5,5), strides = [1, 2, 2, 1], padding='SAME')\n",
        "    d_conv2 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(d_conv2, decay=0.9, epsilon=1e-5), alpha = 0.2)\n",
        "\n",
        "    #Third Convolution Layer\n",
        "    d_conv3 = tf.nn.conv2d(d_conv2, (5,5), strides = [1, 2, 2, 1], padding='SAME')\n",
        "    d_conv3 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(d_conv3, decay=0.9, epsilon=1e-5), alpha = 0.2)\n",
        "\n",
        "    #Fourth Convolution Layer\n",
        "    d_conv4 = tf.nn.conv2d(d_conv3, (5,5), strides = [1, 2, 2, 1], padding='SAME')\n",
        "    d_conv4 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(d_conv4, decay=0.9, epsilon=1e-5), alpha = 0.2)\n",
        "\n",
        "    output  = tf.reshape(d_conv4, [10000, 10]) #No sigmoid in WGAN\n",
        "\n",
        "    return output\n",
        "\n",
        "  def wgan_generator_model():\n",
        "    with tf.variable_scope(\"dcgan_generator_model\") as scope:\n",
        "      if reuse:\n",
        "        scope.reuse_variables()\n",
        "\n",
        "      wt = tf.random_normal_initializer(stddev=0.02)\n",
        "      wt1 = tf.random_normal_initializer(stddev=0.02)\n",
        "      wt2 = tf.random_normal_initializer(stddev=0.02)\n",
        "      wt3 = tf.random_normal_initializer(stddev=0.02)\n",
        "      wt4 = tf.random_normal_initializer(stddev=0.02)\n",
        "\n",
        "    weighted_input = tf.nn.relu(tf.matmul(input, wt))\n",
        "\n",
        "    weighted_input = tf.reshape(weighted_input, [None, 32, 32, 3])  #this should be converted to 4d tensor of type float and shape\n",
        "   \n",
        "    #tf.nn.conv2d_transpose(input, filters, output_shape, strides, padding='SAME', data_format='NHWC',dilations=None, name=None)\n",
        "    #First Convolution Layer\n",
        "    g_conv_layer1 = tf.nn.conv2d_transpose(weighted_input, (5,5), [10000, 32, 32, 3], [1,2,2,1]) #Added by Abhinav \n",
        "    g_conv_layer1 = tf.nn.relu(tf.contrib.layers.batch_norm(g_conv_layer1,decay = 0.9, epsilon=1e-5))\n",
        "\n",
        "    #Second Convolution Layer\n",
        "    g_conv_layer2 = tf.nn.conv2d_transpose(g_conv_layer1, (5,5), [10000, 32, 32, 3], [1,2,2,1])\n",
        "    g_conv_layer2 = tf.nn.relu(tf.contrib.layers.batch_norm(g_conv_layer2,decay = 0.9, epsilon=1e-5))\n",
        "\n",
        "    #Thrid Convolution Layer\n",
        "    g_conv_layer3 = tf.nn.conv2d_transpose(g_conv_layer2, (5,5), [10000, 32, 32, 3], [1,2,2,1])\n",
        "    g_conv_layer3 = tf.nn.relu(tf.contrib.layers.batch_norm(g_conv_layer3,decay = 0.9, epsilon=1e-5))\n",
        "\n",
        "    #Fourth Convolution Layer\n",
        "    g_conv_layer4 = tf.nn.conv2d_transpose(g_conv_layer3, (5,5), [10000, 32, 32, 3], [1,2,2,1])\n",
        "    g_conv_layer4 = tf.nn.relu(tf.contrib.layers.batch_norm(g_conv_layer4,decay = 0.9, epsilon=1e-5))\n",
        "\n",
        "    output = tf.nn.tanh(g_conv_layer4) #if the dataset is the MNIST\n",
        "\n",
        "    return output\n",
        "\n",
        "  def loss(self, x, z):\n",
        "    g_output = self.dcgan_generator_model(z)\n",
        "    d_fake = self.dcgan_discriminator_model(g_output, reuse= False)\n",
        "    d_real = self.dcgan_discriminator_model(x, reuse = True)\n",
        "\n",
        "    d_loss = tf.reduce_mean(d_real) - tf.reduce_mean(d_fake)\n",
        "    g_loss = -tf.reduce_mean(d_fake)\n",
        "\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  def optimizer(self, d_loss, g_loss):\n",
        "    d_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
        "    g_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n",
        "\n",
        "    d_opt = tf.train.RMSPropOptimizer(0.0002, beta1 = 0.5).minimize(-d_loss,\n",
        "                                                            var_list=d_var_list)\n",
        "    g_opt = tf.train.RMSPropOptimizer(0.0002, beta1 = 0.5).minimize(g_loss,\n",
        "                                                            var_list=g_var_list)\n",
        "    return d_opt, g_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgvt81mijjeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #getting the training batch\n",
        "  \n",
        "  features, labels = training_batches()\n",
        "\n",
        "  input_x = tf.reshape(features, [10000, 32, 32, 3])\n",
        "  dcgan = DCGAN()\n",
        "  x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
        "  z = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='noise')\n",
        "  #x, z, probability = dcgan.inputs()\n",
        "  print(x.shape)\n",
        "  disc_loss, gen_loss = dcgan.loss(x, z)\n",
        "  disc_opt, gen_opt = dcgan.optimizer(disc_loss, g_loss)\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(init)\n",
        "  writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #Generator noise to feed the discriminator\n",
        "    z_noise = np.random.uniform(-1.,1., size = [10000, 3072])\n",
        "    for i in range(5):\n",
        "      _, Disc_loss_epoch = sess.run([disc_opt, disc_loss], feed_dict={x:input_x, z:z_noise})\n",
        "\n",
        "    _, Gen_loss_epoch = sess.run([gen_opt, gen_loss], feed_dict = {z_input:z_noise})\n",
        "\n",
        "    #running the Discriminatory summary\n",
        "    #summary_Disc_Loss = sess.run(Disc_loss_total, feed_dict = (x_input:x_batch,z_input:z_noise))\n",
        "    #Adding the Generator summary\n",
        "    writer.add_summary(Disc_loss_epoch, epoch)\n",
        "\n",
        "    #Running the Generator summary\n",
        "    #summary_Gen_Loss = sess.run(Gen_loss_total, feed_dict = (z_input:z_noise))\n",
        "    #Adding the Generator summary\n",
        "    writer.add_summary(Gen_loss_epoch, epoch)\n",
        "\n",
        "    if epoch % 2000 == 0:\n",
        "      print(\"steps: {0} : Generator_loss:{1} : Discriminator_loss:{2}\".format(epoch,Gen_loss_epoch,Disc_loss_epoch))\n",
        "\n",
        "\n",
        "    #Testing \n",
        "    #Generate images from noise, using the generator network\n",
        "    n=6\n",
        "    canvas = np.empty((28*n, 28*n))\n",
        "    for i in range(n):\n",
        "      #Noise input\n",
        "      z_noise = tf.random.uniform(-1.,1., size=[batch_size, z_noise_dim])\n",
        "      #Generate image from noise\n",
        "      g=sess.run(output_Gen, feed_dict={z_input:z_noise})\n",
        "      #Reverse colors for better display\n",
        "      g = -1 * (g-1)\n",
        "      for j in range(n):\n",
        "        #Draw the generated digits\n",
        "        canvas[i * 28:(i+1)*28, j * 28:(j+1)*28] = g[j].reshape([28,28])\n",
        "  \n",
        "    plt.figure(figsize=(n,n))\n",
        "    plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}